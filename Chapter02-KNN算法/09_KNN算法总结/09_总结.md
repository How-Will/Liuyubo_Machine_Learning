# KNN总结

1. 优点
   - 可以解决分类问题，并且天然可以解决多分类问题。
   - 思想简单，效果十分强大
   - 还可用于解决回归问题。scikit-learn中封装了KNeighborsRegressor，用于解决回归问题。
2. 缺点
   - 效率低下。假设有m个样本，n个特征，则预测每一个新的数据需要O(m*n)
   - 高度数据相关。假设要预测的样本周边的出现了错误的值，那么会影响预测的结果。
   - 预测结果不具有可解释性。只是根据距离等因素推断样本所属的类，至于它为什么会属于该类，不能做出解释。
   - 维数灾难。随着维度的增加，“看似相近”的两个点之间的距离越来越大。比如，1维时，0到1的距离是1，2维时，（0，0）到（1，1）的距离是1.414，3维时(0,0,0)到(1,1,1)的距离是1.73.可以用降维解决

# 机器学习流程

![机器学习流程](D:/Download/下载.png)

